import streamlit as st
import pandas as pd
import json
from datetime import datetime
from app import RFPAnalyzer
import fitz  # PyMuPDF
import docx  # python-docx

# í˜ì´ì§€ ì„¤ì • ë° ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
st.set_page_config(page_title="RFP ë¶„ì„ ëŒ€ì‹œë³´ë“œ", layout="wide")

if "search_history" not in st.session_state:
    st.session_state.search_history = []

st.title("RFP ë¶„ì„ ìë™í™” â€” Streamlit UI")

st.markdown(
    """
    ì´ UIëŠ” Azure Searchì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•„ Azure OpenAIë¡œë¶€í„° êµ¬ì¡°í™”ëœ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    ### ê¸°ëŠ¥
    - ğŸ” ë¬¸ì„œ ê²€ìƒ‰ ë° AI ê¸°ë°˜ ë¶„ì„
    - ğŸ’¡ í‚¤ì›Œë“œ í•˜ì´ë¼ì´íŠ¸
    - ğŸ“Š ê²€ìƒ‰ ê²°ê³¼ ì •ë ¬/í•„í„°ë§
    - ğŸ’¾ JSON í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ ì €ì¥
    - ğŸ“œ ê²€ìƒ‰ íˆìŠ¤í† ë¦¬ ê´€ë¦¬
    """
)

def highlight_text(text, keywords):
    """í…ìŠ¤íŠ¸ ë‚´ í‚¤ì›Œë“œë¥¼ í•˜ì´ë¼ì´íŠ¸"""
    if not text or not keywords:
        return text
    
    highlighted = text
    for kw in keywords:
        if kw and kw in str(highlighted):
            highlighted = str(highlighted).replace(
                kw, f"**:red[{kw}]**"
            )
    return highlighted

def save_to_json(data, filename_prefix="rfp_search"):
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{filename_prefix}_{timestamp}.json"
    
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    return filename

def extract_pdf_text(file):
    """PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        with fitz.open(stream=file) as doc:
            text = ""
            for page in doc:
                text += page.get_text()
        return text
    except Exception as e:
        return f"(PDF íŒŒì‹± ì˜¤ë¥˜: {e})"

def extract_docx_text(file):
    """DOCX íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        doc = docx.Document(file)
        text = "\n".join([p.text for p in doc.paragraphs])
        return text
    except Exception as e:
        return f"(DOCX íŒŒì‹± ì˜¤ë¥˜: {e})"

with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    
    # ê²€ìƒ‰ ì„¤ì •
    st.subheader("ê²€ìƒ‰ ì˜µì…˜")
    top_n = st.number_input("ê°€ì ¸ì˜¬ ë¬¸ì„œ ìˆ˜ (top N)", min_value=1, max_value=20, value=5)
    
    # í‚¤ì›Œë“œ í•˜ì´ë¼ì´íŠ¸
    st.subheader("í‚¤ì›Œë“œ í•˜ì´ë¼ì´íŠ¸")
    highlight_keywords = st.text_input(
        "í•˜ì´ë¼ì´íŠ¸í•  í‚¤ì›Œë“œ (ì‰¼í‘œë¡œ êµ¬ë¶„)",
        placeholder="ì˜ˆ: AI, í´ë¼ìš°ë“œ, ë³´ì•ˆ"
    ).split(",")
    highlight_keywords = [k.strip() for k in highlight_keywords if k.strip()]
    
    # ì •ë ¬ ì˜µì…˜
    st.subheader("ì •ë ¬ ì˜µì…˜")
    sort_by = st.selectbox(
        "ì •ë ¬ ê¸°ì¤€",
        ["ì—°ê´€ë„", "ì¤‘ìš”ë„", "í”„ë¡œì íŠ¸ëª…"],
        index=0
    )
    
    # í•„í„° ì˜µì…˜
    st.subheader("í•„í„°")
    min_importance = st.slider("ìµœì†Œ ì¤‘ìš”ë„", 0.0, 1.0, 0.0)
    
    # ì‹¤í–‰ ë²„íŠ¼
    run_button = st.button("ğŸ” ê²€ìƒ‰ ì‹¤í–‰")

# ê²€ìƒ‰ íˆìŠ¤í† ë¦¬
if st.session_state.search_history:
    with st.expander("ğŸ“œ ê²€ìƒ‰ íˆìŠ¤í† ë¦¬"):
        for hist in st.session_state.search_history[-5:]:  # ìµœê·¼ 5ê°œë§Œ
            st.text(f"[{hist['timestamp']}] {hist['query'][:50]}...")

# ì¿¼ë¦¬ ì…ë ¥ ì„¹ì…˜
st.header("ğŸ“ ì¿¼ë¦¬ ì…ë ¥")

col1, col2 = st.columns([3, 1])

with col2:
    # define default query here so it's available to the button handler before text_area
    query_default = (
        "RFP ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì‚¬ì—…ëª…, ì‚¬ì—…ê¸°ê°„, ì‚¬ì—…ëª©ì , ì‚¬ì—…ë²”ìœ„, í•µì‹¬ê¸°ìˆ , ê³ ê°ì‚¬ëª…, "
        "ì‚¬ì—…ì„¤ëª…íšŒë‚ ì§œ, ì…ì°°ì¼ì, PTë°œí‘œì¼, ìš°ì„ í˜‘ìƒëŒ€ìƒì ì„ ì • ë°œí‘œì¼, ì œì•½ì‚¬í•­ì„ ì•Œë ¤ì£¼ì„¸ìš”."
    )

    # ê¸°ë³¸ ì¿¼ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸° ë™ì‘: ë²„íŠ¼ í´ë¦­ ì‹œ session_stateì— ê¸°ë¡
    if st.button("ğŸ“‹ ê¸°ë³¸ ì¿¼ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°", help="í´ë¦­í•˜ë©´ ê¸°ë³¸ ì¿¼ë¦¬ë¥¼ í”„ë¡¬í”„íŠ¸ì— ì±„ì›ë‹ˆë‹¤", key="prefill_button"):
        st.session_state.query = query_default
        st.session_state.prefilled = True

    # ë³´ì—¬ì§„ ê¸°ë³¸ì¿¼ë¦¬ë¡œ ì¦‰ì‹œ ì‹¤í–‰í• ì§€ ì˜µì…˜ (ê¸°ë³¸ ì¿¼ë¦¬ë¥¼ ë¶ˆëŸ¬ì˜¨ ê²½ìš°ì—ë§Œ í‘œì‹œ)
    # ì¿¼ë¦¬ ì‹¤í–‰ ë²„íŠ¼ì€ í•­ìƒ í‘œì‹œ
    if st.button("ğŸš€ ì¿¼ë¦¬ ì‹¤í–‰", key="execute_loaded_query", help="í˜„ì¬ ì…ë ¥ëœ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."):
        st.session_state.use_default_execute = True
    else:
        st.session_state.use_default_execute = False

with col1:
    # í…ìŠ¤íŠ¸ ì˜ì—­ì€ ì„¸ì…˜ ìƒíƒœ 'query'ë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ê°€ëŠ¥í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.
    if "query" not in st.session_state:
        st.session_state.query = ""

    # text_areaëŠ” keyë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒíƒœë¥¼ ìë™ìœ¼ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.
    query = st.text_area("ì‚¬ìš©ì ì§ˆë¬¸/ì¿¼ë¦¬", value=st.session_state.get("query", ""), key="query", height=160, placeholder="ì—¬ê¸°ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")

# ì‹¤í–‰ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤„ ì»¨í…Œì´ë„ˆ
results_container = st.container()

# ê²€ìƒ‰ ì‹¤í–‰ ì„¹ì…˜: ê²€ìƒ‰ë§Œ ìˆ˜í–‰í•˜ê³  ë¬¸ì„œ ëª©ë¡ì„ ë¨¼ì € í‘œì‹œí•©ë‹ˆë‹¤ (LLM í˜¸ì¶œì€ ë³„ë„)
# ì‹¤í–‰ ì¡°ê±´: ì¼ë°˜ ì‹¤í–‰ ë²„íŠ¼ ë˜ëŠ” (ê¸°ë³¸ì¿¼ë¦¬(prefill) í›„ ê¸°ë³¸ì¿¼ë¦¬ë¡œ ì¦‰ì‹œ ì‹¤í–‰ ì˜µì…˜ ì²´í¬)
use_prefill_and_execute = st.session_state.get("prefilled", False) and st.session_state.get("use_default_execute", False)
if run_button or use_prefill_and_execute:
    with results_container:
        if not query.strip():
            st.error("ğŸš« ì¿¼ë¦¬ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ ê¸°ë³¸ ì¿¼ë¦¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
        else:
            with st.spinner("ğŸ”„ ê²€ìƒ‰ ì¤‘... Azure Search í˜¸ì¶œì„ ì‹¤í–‰í•©ë‹ˆë‹¤"):
                try:
                    analyzer = RFPAnalyzer()
                    raw_docs = analyzer.search(query, top=int(top_n))

                    # Convert to plain dicts for session storage and display
                    raw_docs = [dict(d) for d in raw_docs]

                    # ê²€ìƒ‰ íˆìŠ¤í† ë¦¬ ì €ì¥
                    st.session_state.search_history.append({
                        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        "query": query,
                        "n_results": len(raw_docs)
                    })

                    # Prepare display-friendly docs (Korean keys)
                    docs_list = []
                    for d in raw_docs:
                        doc_dict = {
                            "í”„ë¡œì íŠ¸ëª…": d.get("projectName"),
                            "ì¤‘ìš”ë„": float(d.get("importance", 0) or 0),
                            "ê¸°ëŠ¥ìš”êµ¬ì‚¬í•­": d.get("functionalRequirements"),
                            "ë¹„ê¸°ëŠ¥ìš”êµ¬ì‚¬í•­": d.get("nonFunctionalRequirements"),
                            "ê¸°ìˆ ìš”êµ¬ì‚¬í•­": d.get("technicalRequirements"),
                            "ìŠ¤í‚¬ì…‹": d.get("skillsets", []),
                            "ë³¸ë¬¸": d.get("chunk")
                        }
                        if float(doc_dict["ì¤‘ìš”ë„"]) >= min_importance:
                            docs_list.append(doc_dict)

                    # ì •ë ¬ ì ìš©
                    if docs_list:
                        if sort_by == "ì¤‘ìš”ë„":
                            docs_list.sort(key=lambda x: x["ì¤‘ìš”ë„"], reverse=True)
                        elif sort_by == "í”„ë¡œì íŠ¸ëª…":
                            docs_list.sort(key=lambda x: x["í”„ë¡œì íŠ¸ëª…"] or "")

                    # ì €ì¥: ì›ë³¸(raw_docs)ê³¼ í‘œì‹œìš©(docs_list)
                    st.session_state.last_raw_docs = raw_docs
                    st.session_state.last_display_docs = docs_list

                    st.success(f"âœ… ê²€ìƒ‰ ì™„ë£Œ â€” {len(docs_list)}ê°œ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

                except Exception as e:
                    st.error(f"âŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                    if "Missing Azure" in str(e):
                        st.warning("âš ï¸ í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")

# ë¬¸ì„œê°€ ìˆìœ¼ë©´ í‘œì‹œ
if "last_display_docs" in st.session_state and st.session_state.last_display_docs:
    st.subheader("ğŸ“‘ ê²€ìƒ‰ëœ ë¬¸ì„œ (ìš”ì•½)")
    for i, d in enumerate(st.session_state.last_display_docs, start=1):
        with st.expander(f"ë¬¸ì„œ {i}: {d['í”„ë¡œì íŠ¸ëª…'] or 'ì œëª© ì—†ìŒ'} | ì¤‘ìš”ë„: {d['ì¤‘ìš”ë„']:.2f}"):
            cols = st.columns(2)
            with cols[0]:
                st.markdown("### ìš”êµ¬ì‚¬í•­")
                st.markdown("**ğŸ”¹ ê¸°ëŠ¥ ìš”êµ¬ì‚¬í•­**")
                st.markdown(highlight_text(d["ê¸°ëŠ¥ìš”êµ¬ì‚¬í•­"], highlight_keywords))
                st.markdown("**ğŸ”¹ ë¹„ê¸°ëŠ¥ ìš”êµ¬ì‚¬í•­**")
                st.markdown(highlight_text(d["ë¹„ê¸°ëŠ¥ìš”êµ¬ì‚¬í•­"], highlight_keywords))
                st.markdown("**ğŸ”¹ ê¸°ìˆ  ìš”êµ¬ì‚¬í•­**")
                st.markdown(highlight_text(d["ê¸°ìˆ ìš”êµ¬ì‚¬í•­"], highlight_keywords))
            with cols[1]:
                st.markdown("### ìŠ¤í‚¬ì…‹ ë° ë³¸ë¬¸")
                if d["ìŠ¤í‚¬ì…‹"]:
                    st.markdown("**ğŸ”¹ í•„ìš” ìŠ¤í‚¬**")
                    skillset_str = ", ".join(d["ìŠ¤í‚¬ì…‹"]) if isinstance(d["ìŠ¤í‚¬ì…‹"], list) else str(d["ìŠ¤í‚¬ì…‹"])
                    st.markdown(highlight_text(skillset_str, highlight_keywords))
                st.markdown("**ğŸ”¹ ë³¸ë¬¸ ë‚´ìš©**")
                st.markdown(highlight_text(d["ë³¸ë¬¸"], highlight_keywords))

    # --- ì¶”ê°€ íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜ (AI ë¶„ì„ ìœ„) ---
    st.markdown("---")
    st.subheader("ğŸ“ ì¶”ê°€ ë¬¸ì„œ ì—…ë¡œë“œ (ì„ íƒì‚¬í•­)")
    uploaded_files = st.file_uploader(
        "ë¶„ì„ì— í¬í•¨í•  ì¶”ê°€ ë¬¸ì„œë¥¼ ì„ íƒí•˜ì„¸ìš” (ì—¬ëŸ¬ íŒŒì¼ ì„ íƒ ê°€ëŠ¥)",
        accept_multiple_files=True,
        type=['txt', 'pdf', 'docx']
    )
    uploaded_docs = []
    if uploaded_files:
        for file in uploaded_files:
            if file.type == "text/plain":
                content = file.read().decode("utf-8")
            elif file.type == "application/pdf":
                content = extract_pdf_text(file)  # ì‹¤ì œ PDF íŒŒì‹±ì€ ë³„ë„ êµ¬í˜„ í•„ìš”
            elif file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
                content = extract_docx_text(file)  # ì‹¤ì œ DOCX íŒŒì‹±ì€ ë³„ë„ êµ¬í˜„ í•„ìš”
            else:
                content = "(ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼ í˜•ì‹)"
            uploaded_docs.append({
                "í”„ë¡œì íŠ¸ëª…": file.name,
                "chunk": content,
                "ê¸°ëŠ¥ìš”êµ¬ì‚¬í•­": "",
                "ë¹„ê¸°ëŠ¥ìš”êµ¬ì‚¬í•­": "",
                "ê¸°ìˆ ìš”êµ¬ì‚¬í•­": "",
                "ìŠ¤í‚¬ì…‹": [],
                "ì¤‘ìš”ë„": 0.0
            })
        st.session_state.uploaded_docs = uploaded_docs
    else:
        st.session_state.uploaded_docs = []

    # --- LLM í”„ë¡¬í”„íŠ¸ ì…ë ¥ ë° ì‹¤í–‰ ì„¹ì…˜ ---
    st.markdown("---")
    st.header("ğŸ¤– AI ë¶„ì„")
    st.write("ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ì—…ë¡œë“œëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLMì—ê²Œ ì¶”ê°€ ì§ˆì˜ë¥¼ í•˜ë ¤ë©´ ì•„ë˜ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ê³  'AI ë¶„ì„ ìƒì„±' ë²„íŠ¼ì„ ëˆ„ë¥´ì„¸ìš”.")
    llm_prompt = st.text_area("LLMì— ë³´ë‚¼ ì§ˆë¬¸/í”„ë¡¬í”„íŠ¸", value="RFP ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì£¼ìš” ìš”êµ¬ì‚¬í•­ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”.", height=120)
    # ê¸°ì¡´ ê²€ìƒ‰ ë¬¸ì„œ + ì—…ë¡œë“œ ë¬¸ì„œ í•©ì¹˜ê¸°
    all_docs = st.session_state.get("last_raw_docs", []) + st.session_state.get("uploaded_docs", [])
    doc_labels = []
    filtered_docs = []
    for i, doc in enumerate(all_docs):
        label = doc.get("í”„ë¡œì íŠ¸ëª…") or doc.get("projectName")
        if label:  # ë¬¸ì„œëª…ì´ ìˆìœ¼ë©´ë§Œ ì˜µì…˜ì— ì¶”ê°€
            doc_labels.append(label)
            filtered_docs.append(doc)
            
    selected_labels = st.multiselect(
        "ë¶„ì„ì— í¬í•¨í•  ë¬¸ì„œ ì„ íƒ",
        options=doc_labels,
        default=doc_labels,
        help="ë¶„ì„ì— í¬í•¨í•  ë¬¸ì„œë¥¼ ì„ íƒí•˜ì„¸ìš”. ê¸°ë³¸ì ìœ¼ë¡œ ëª¨ë“  ë¬¸ì„œê°€ ì„ íƒë©ë‹ˆë‹¤."
    )
    selected_docs = [doc for doc, label in zip(all_docs, doc_labels) if label in selected_labels]
    
    
    gen_button = st.button("ğŸ§  AI ë¶„ì„ ìƒì„±")

    if gen_button:
        if not llm_prompt.strip():
            st.error("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            with st.spinner("LLM í˜¸ì¶œ ì¤‘... ì ì‹œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”"):
                try:
                    analyzer = RFPAnalyzer()
                    response_text = analyzer.generate_from_documents(selected_docs, prompt=llm_prompt)
                    st.subheader("ğŸ¤– LLM ì‘ë‹µ")
                    st.info(highlight_text(response_text, highlight_keywords))
                    st.session_state.last_llm_response = response_text
                except Exception as e:
                    st.error(f"LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
else:
    st.info("ê²€ìƒ‰ì„ ë¨¼ì € ì‹¤í–‰í•˜ë©´ ë¬¸ì„œ ëª©ë¡ì´ ì—¬ê¸° í‘œì‹œë©ë‹ˆë‹¤. ê·¸ ë‹¤ìŒ LLMì— ì§ˆë¬¸ì„ ë³´ë‚´ ì¶”ê°€ ë¶„ì„ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

st.markdown("---")
st.caption("í™˜ê²½ë³€ìˆ˜: AZURE_SEARCH_API_KEY, AZURE_SEARCH_ENDPOINT, AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, AZURE_DEPLOYMENT_MODEL í•„ìš”")